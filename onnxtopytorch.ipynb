{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  onnx\n",
    "import onnx2pytorch\n",
    "from onnx2pytorch import ConvertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = onnx.load('models/onnxvgg16.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\pytorchgpu\\lib\\site-packages\\onnx2pytorch\\convert\\layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:212.)\n",
      "  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Conversion not implemented for op_type=GlobalMaxPool.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pytorch_model \u001b[39m=\u001b[39m ConvertModel(model)\n",
      "File \u001b[1;32mc:\\anaconda3\\envs\\pytorchgpu\\lib\\site-packages\\onnx2pytorch\\convert\\model.py:122\u001b[0m, in \u001b[0;36mConvertModel.__init__\u001b[1;34m(self, onnx_model, batch_dim, experimental, debug, enable_pruning)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[39m# Create mapping from node (identified by first output) to submodule\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmapping \u001b[39m=\u001b[39m {}\n\u001b[1;32m--> 122\u001b[0m \u001b[39mfor\u001b[39;00m op_id, op_name, op \u001b[39min\u001b[39;00m convert_operations(\n\u001b[0;32m    123\u001b[0m     onnx_model\u001b[39m.\u001b[39mgraph,\n\u001b[0;32m    124\u001b[0m     opset_version,\n\u001b[0;32m    125\u001b[0m     batch_dim,\n\u001b[0;32m    126\u001b[0m     enable_pruning,\n\u001b[0;32m    127\u001b[0m ):\n\u001b[0;32m    128\u001b[0m     \u001b[39msetattr\u001b[39m(\u001b[39mself\u001b[39m, op_name, op)\n\u001b[0;32m    129\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(op, Loop) \u001b[39mand\u001b[39;00m debug:\n",
      "File \u001b[1;32mc:\\anaconda3\\envs\\pytorchgpu\\lib\\site-packages\\onnx2pytorch\\convert\\operations.py:279\u001b[0m, in \u001b[0;36mconvert_operations\u001b[1;34m(onnx_graph, opset_version, batch_dim, enable_pruning)\u001b[0m\n\u001b[0;32m    277\u001b[0m op \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(torch, node\u001b[39m.\u001b[39mop_type\u001b[39m.\u001b[39mlower(), \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    278\u001b[0m \u001b[39mif\u001b[39;00m op \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 279\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConversion not implemented for op_type=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(node\u001b[39m.\u001b[39mop_type)\n\u001b[0;32m    281\u001b[0m     )\n\u001b[0;32m    282\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    283\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    284\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAutomatic inference of operator: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(node\u001b[39m.\u001b[39mop_type\u001b[39m.\u001b[39mlower())\n\u001b[0;32m    285\u001b[0m     )\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Conversion not implemented for op_type=GlobalMaxPool."
     ]
    }
   ],
   "source": [
    "pytorch_model = ConvertModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'onnx2pytorch' has no attribute 'convert_from_onnx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pytorch_model \u001b[39m=\u001b[39m onnx2pytorch\u001b[39m.\u001b[39;49mconvert_from_onnx(model)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'onnx2pytorch' has no attribute 'convert_from_onnx'"
     ]
    }
   ],
   "source": [
    "pytorch_model = onnx2pytorch.convert_from_onnx(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf.h5\n",
      "17227776/17225924 [==============================] - 96s 6us/step\n",
      "17235968/17225924 [==============================] - 96s 6us/step\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "\n",
    "# Load MobileNetV1 with pre-trained weights on ImageNet\n",
    "model = MobileNet(weights='imagenet')\n",
    "\n",
    "# Save the model\n",
    "model.save('mobilenetv1_imagenet.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('mobilenetv1_imagenet.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf2onnx\n",
    "import onnx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = tf2onnx.convert.from_keras(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_proto, _ = tf2onnx.convert.from_keras(model)\n",
    "onnx.save(model_proto, 'mobilenetv1_onnx.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnx2pytorch\n",
    "import torch\n",
    "from onnx2pytorch import ConvertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = 'mobilenetv1_onnx.onnx'\n",
    "onnx_model = onnx.load(onnx_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\pytorchgpu\\lib\\site-packages\\onnx2pytorch\\convert\\layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:212.)\n",
      "  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n"
     ]
    }
   ],
   "source": [
    "# Convert ONNX model to PyTorch\n",
    "pytorch_model = ConvertModel(onnx_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the PyTorch model\n",
    "torch.save(pytorch_model.state_dict(), 'pytorchmobilenetv1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import onnx\n",
    "\n",
    "# Load PyTorch model\n",
    "model = torch.load(\"pytorchmobilenetv1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'onnx' has no attribute 'export'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m dummy_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m224\u001b[39m, \u001b[39m224\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m onnx_model \u001b[39m=\u001b[39m onnx\u001b[39m.\u001b[39;49mexport(model, dummy_input, \u001b[39m\"\u001b[39m\u001b[39mmy_model.onnx\u001b[39m\u001b[39m\"\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'onnx' has no attribute 'export'"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "onnx_model = onnx.export(model, dummy_input, \"my_model.onnx\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (1, 3, 224, 224)  # (batch_size, channels, height, width)\n",
    "input_tensor = torch.randn(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'modules'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m output_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpytorchmobilenetv1.onnx\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m torch\u001b[39m.\u001b[39;49monnx\u001b[39m.\u001b[39;49mexport(model, input_tensor, output_path)\n",
      "File \u001b[1;32mc:\\anaconda3\\envs\\pytorchgpu\\lib\\site-packages\\torch\\onnx\\utils.py:506\u001b[0m, in \u001b[0;36mexport\u001b[1;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[39m@_beartype\u001b[39m\u001b[39m.\u001b[39mbeartype\n\u001b[0;32m    189\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexport\u001b[39m(\n\u001b[0;32m    190\u001b[0m     model: Union[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptFunction],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    206\u001b[0m     export_modules_as_functions: Union[\u001b[39mbool\u001b[39m, Collection[Type[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule]]] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    207\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \n\u001b[0;32m    210\u001b[0m \u001b[39m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[39m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[0;32m    504\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 506\u001b[0m     _export(\n\u001b[0;32m    507\u001b[0m         model,\n\u001b[0;32m    508\u001b[0m         args,\n\u001b[0;32m    509\u001b[0m         f,\n\u001b[0;32m    510\u001b[0m         export_params,\n\u001b[0;32m    511\u001b[0m         verbose,\n\u001b[0;32m    512\u001b[0m         training,\n\u001b[0;32m    513\u001b[0m         input_names,\n\u001b[0;32m    514\u001b[0m         output_names,\n\u001b[0;32m    515\u001b[0m         operator_export_type\u001b[39m=\u001b[39;49moperator_export_type,\n\u001b[0;32m    516\u001b[0m         opset_version\u001b[39m=\u001b[39;49mopset_version,\n\u001b[0;32m    517\u001b[0m         do_constant_folding\u001b[39m=\u001b[39;49mdo_constant_folding,\n\u001b[0;32m    518\u001b[0m         dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[0;32m    519\u001b[0m         keep_initializers_as_inputs\u001b[39m=\u001b[39;49mkeep_initializers_as_inputs,\n\u001b[0;32m    520\u001b[0m         custom_opsets\u001b[39m=\u001b[39;49mcustom_opsets,\n\u001b[0;32m    521\u001b[0m         export_modules_as_functions\u001b[39m=\u001b[39;49mexport_modules_as_functions,\n\u001b[0;32m    522\u001b[0m     )\n",
      "File \u001b[1;32mc:\\anaconda3\\envs\\pytorchgpu\\lib\\site-packages\\torch\\onnx\\utils.py:1525\u001b[0m, in \u001b[0;36m_export\u001b[1;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions)\u001b[0m\n\u001b[0;32m   1520\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(model, (torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptModule)):\n\u001b[0;32m   1521\u001b[0m     module_typenames_to_export_as_functions \u001b[39m=\u001b[39m _setup_trace_module_map(\n\u001b[0;32m   1522\u001b[0m         model, export_modules_as_functions\n\u001b[0;32m   1523\u001b[0m     )\n\u001b[1;32m-> 1525\u001b[0m \u001b[39mwith\u001b[39;00m exporter_context(model, training, verbose):\n\u001b[0;32m   1526\u001b[0m     val_keep_init_as_ip \u001b[39m=\u001b[39m _decide_keep_init_as_input(\n\u001b[0;32m   1527\u001b[0m         keep_initializers_as_inputs,\n\u001b[0;32m   1528\u001b[0m         operator_export_type,\n\u001b[0;32m   1529\u001b[0m         opset_version,\n\u001b[0;32m   1530\u001b[0m     )\n\u001b[0;32m   1531\u001b[0m     val_add_node_names \u001b[39m=\u001b[39m _decide_add_node_names(\n\u001b[0;32m   1532\u001b[0m         add_node_names, operator_export_type\n\u001b[0;32m   1533\u001b[0m     )\n",
      "File \u001b[1;32mc:\\anaconda3\\envs\\pytorchgpu\\lib\\contextlib.py:119\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwds, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc\n\u001b[0;32m    118\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 119\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen)\n\u001b[0;32m    120\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    121\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgenerator didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt yield\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\anaconda3\\envs\\pytorchgpu\\lib\\site-packages\\torch\\onnx\\utils.py:178\u001b[0m, in \u001b[0;36mexporter_context\u001b[1;34m(model, mode, verbose)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[39m@contextlib\u001b[39m\u001b[39m.\u001b[39mcontextmanager\n\u001b[0;32m    176\u001b[0m \u001b[39m@_beartype\u001b[39m\u001b[39m.\u001b[39mbeartype\n\u001b[0;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexporter_context\u001b[39m(model, mode: _C_onnx\u001b[39m.\u001b[39mTrainingMode, verbose: \u001b[39mbool\u001b[39m):\n\u001b[1;32m--> 178\u001b[0m     \u001b[39mwith\u001b[39;00m select_model_mode_for_export(\n\u001b[0;32m    179\u001b[0m         model, mode\n\u001b[0;32m    180\u001b[0m     ) \u001b[39mas\u001b[39;00m mode_ctx, disable_apex_o2_state_dict_hook(\n\u001b[0;32m    181\u001b[0m         model\n\u001b[0;32m    182\u001b[0m     ) \u001b[39mas\u001b[39;00m apex_ctx, setup_onnx_logging(\n\u001b[0;32m    183\u001b[0m         verbose\n\u001b[0;32m    184\u001b[0m     ) \u001b[39mas\u001b[39;00m log_ctx, diagnostics\u001b[39m.\u001b[39mcreate_export_diagnostic_context() \u001b[39mas\u001b[39;00m diagnostic_ctx:\n\u001b[0;32m    185\u001b[0m         \u001b[39myield\u001b[39;00m (mode_ctx, apex_ctx, log_ctx, diagnostic_ctx)\n",
      "File \u001b[1;32mc:\\anaconda3\\envs\\pytorchgpu\\lib\\contextlib.py:119\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwds, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc\n\u001b[0;32m    118\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 119\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen)\n\u001b[0;32m    120\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    121\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgenerator didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt yield\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\anaconda3\\envs\\pytorchgpu\\lib\\site-packages\\torch\\onnx\\utils.py:139\u001b[0m, in \u001b[0;36mdisable_apex_o2_state_dict_hook\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(model, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptFunction):\n\u001b[0;32m    138\u001b[0m     model_hooks \u001b[39m=\u001b[39m {}  \u001b[39m# type: ignore[var-annotated]\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39;49mmodules():\n\u001b[0;32m    140\u001b[0m         \u001b[39mfor\u001b[39;00m key, hook \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39m_state_dict_hooks\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    141\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(hook)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mO2StateDictHook\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'modules'"
     ]
    }
   ],
   "source": [
    "output_path = \"pytorchmobilenetv1.onnx\"\n",
    "torch.onnx.export(model, input_tensor, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'TFLiteConverterV2' has no attribute 'from_onnx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m onnx_model \u001b[39m=\u001b[39m onnx\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mmobilenetv1_onnx.onnx\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[39m# Convert the ONNX model to a TensorFlow Lite model\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m converter \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mlite\u001b[39m.\u001b[39;49mTFLiteConverter\u001b[39m.\u001b[39;49mfrom_onnx(onnx_model)\n\u001b[0;32m      9\u001b[0m tflite_model \u001b[39m=\u001b[39m converter\u001b[39m.\u001b[39mconvert()\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'TFLiteConverterV2' has no attribute 'from_onnx'"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model = onnx.load('mobilenetv1_onnx.onnx')\n",
    "\n",
    "# Convert the ONNX model to a TensorFlow Lite model\n",
    "converter = tf.lite.TFLiteConverter.from_onnx(onnx_model)\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.lite.python import interpreter as interpreter_wrapper\n",
    "from tensorflow.python.framework import tensor_util\n",
    "from tensorflow.python.platform import gfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = 'mobilenetv1_onnx.onnx'\n",
    "with gfile.GFile(onnx_model_path, 'rb') as onnx_model_file:\n",
    "    onnx_graph = onnx_model_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'TFLiteConverterV2' has no attribute 'from_onnx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tflite_model \u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39;49mlite\u001b[39m.\u001b[39;49mTFLiteConverter\u001b[39m.\u001b[39;49mfrom_onnx(onnx_graph)\u001b[39m.\u001b[39mconvert()\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'TFLiteConverterV2' has no attribute 'from_onnx'"
     ]
    }
   ],
   "source": [
    "tflite_model =tf.lite.TFLiteConverter.from_onnx(onnx_graph).convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "c:\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.10.0 and strictly below 2.13.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.6.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import onnx\n",
    "from onnx_tf.backend import prepare\n",
    "from tensorflow import lite\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model = onnx.load('mobilenetv1_onnx.onnx')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load('mobilenetv1_onnx.onnx')\n",
    "\n",
    "# Convert the ONNX model to TensorFlow format\n",
    "tf2oNnx_model = prepare(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TensorflowRep' object has no attribute 'call'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m converter \u001b[39m=\u001b[39m lite\u001b[39m.\u001b[39mTFLiteConverter\u001b[39m.\u001b[39mfrom_keras_model(tf2oNnx_model)\n\u001b[0;32m      2\u001b[0m converter\u001b[39m.\u001b[39moptimizations \u001b[39m=\u001b[39m [tf\u001b[39m.\u001b[39mlite\u001b[39m.\u001b[39mOptimize\u001b[39m.\u001b[39mDEFAULT]\n\u001b[1;32m----> 3\u001b[0m tflite_model \u001b[39m=\u001b[39m converter\u001b[39m.\u001b[39;49mconvert()\n",
      "File \u001b[1;32mc:\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py:729\u001b[0m, in \u001b[0;36m_export_metrics.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(convert_func)\n\u001b[0;32m    727\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    728\u001b[0m   \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m--> 729\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_and_export_metrics(convert_func, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py:715\u001b[0m, in \u001b[0;36mTFLiteConverterBase._convert_and_export_metrics\u001b[1;34m(self, convert_func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_conversion_params_metric()\n\u001b[0;32m    714\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mprocess_time()\n\u001b[1;32m--> 715\u001b[0m result \u001b[39m=\u001b[39m convert_func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    716\u001b[0m elapsed_time_ms \u001b[39m=\u001b[39m (time\u001b[39m.\u001b[39mprocess_time() \u001b[39m-\u001b[39m start_time) \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[0;32m    717\u001b[0m \u001b[39mif\u001b[39;00m result:\n",
      "File \u001b[1;32mc:\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1123\u001b[0m, in \u001b[0;36mTFLiteKerasModelConverterV2.convert\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1119\u001b[0m \u001b[39mif\u001b[39;00m saved_model_convert_result:\n\u001b[0;32m   1120\u001b[0m   \u001b[39mreturn\u001b[39;00m saved_model_convert_result\n\u001b[0;32m   1122\u001b[0m graph_def, input_tensors, output_tensors, frozen_func \u001b[39m=\u001b[39m (\n\u001b[1;32m-> 1123\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_freeze_keras_model())\n\u001b[0;32m   1125\u001b[0m graph_def \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimize_tf_model(graph_def, input_tensors,\n\u001b[0;32m   1126\u001b[0m                                     output_tensors, frozen_func)\n\u001b[0;32m   1128\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m(TFLiteKerasModelConverterV2,\n\u001b[0;32m   1129\u001b[0m              \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mconvert(graph_def, input_tensors, output_tensors)\n",
      "File \u001b[1;32mc:\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\lite\\python\\convert_phase.py:218\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m error:\n\u001b[0;32m    217\u001b[0m   report_error_message(\u001b[39mstr\u001b[39m(error))\n\u001b[1;32m--> 218\u001b[0m   \u001b[39mraise\u001b[39;00m error \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\lite\\python\\convert_phase.py:208\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    207\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 208\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    209\u001b[0m   \u001b[39mexcept\u001b[39;00m ConverterError \u001b[39mas\u001b[39;00m converter_error:\n\u001b[0;32m    210\u001b[0m     \u001b[39mif\u001b[39;00m converter_error\u001b[39m.\u001b[39merrors:\n",
      "File \u001b[1;32mc:\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1066\u001b[0m, in \u001b[0;36mTFLiteKerasModelConverterV2._freeze_keras_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1060\u001b[0m input_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1061\u001b[0m \u001b[39m# If the model's call is not a `tf.function`, then we need to first get its\u001b[39;00m\n\u001b[0;32m   1062\u001b[0m \u001b[39m# input signature from `model_input_signature` method. We can't directly\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m \u001b[39m# call `trace_model_call` because otherwise the batch dimension is set\u001b[39;00m\n\u001b[0;32m   1064\u001b[0m \u001b[39m# to None.\u001b[39;00m\n\u001b[0;32m   1065\u001b[0m \u001b[39m# Once we have better support for dynamic shapes, we can remove this.\u001b[39;00m\n\u001b[1;32m-> 1066\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_keras_model\u001b[39m.\u001b[39;49mcall, _def_function\u001b[39m.\u001b[39mFunction):\n\u001b[0;32m   1067\u001b[0m   \u001b[39m# Pass `keep_original_batch_size=True` will ensure that we get an input\u001b[39;00m\n\u001b[0;32m   1068\u001b[0m   \u001b[39m# signature including the batch dimension specified by the user.\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m   \u001b[39m# TODO(b/169898786): Use the Keras public API when TFLite moves out of TF\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m   input_signature \u001b[39m=\u001b[39m _model_input_signature(\n\u001b[0;32m   1071\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_keras_model, keep_original_batch_size\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   1073\u001b[0m \u001b[39m# TODO(b/169898786): Use the Keras public API when TFLite moves out of TF\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TensorflowRep' object has no attribute 'call'"
     ]
    }
   ],
   "source": [
    "converter = lite.TFLiteConverter.from_onnx(tf2oNnx_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnx2pytorch\n",
    "import torch\n",
    "from onnx2pytorch import ConvertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
